{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras on Session mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "#import tensorflow.contrib.eager as tfe\n",
    "#tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "training_epochs = 21\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_dir = os.getcwd()\n",
    "ckpt_dir_name = 'checkpoints'\n",
    "model_dir_name = 'mnist_keras_session'\n",
    "\n",
    "checkpoint_dir = os.path.join(cur_dir, ckpt_dir_name, model_dir_name)\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, model_dir_name)\n",
    "\n",
    "## TensorBoard\n",
    "log_dir = os.path.join(cur_dir, 'tensorboard', model_dir_name)\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,) (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "mnist = keras.datasets.mnist\n",
    "\n",
    "(train_x, train_y), (test_x, test_y) = mnist.load_data()\n",
    "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000, 10) (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "train_x = train_x.astype(np.float32) / 255\n",
    "test_x = test_x.astype(np.float32) / 255\n",
    "\n",
    "train_x = np.reshape(train_x, [-1,28*28])\n",
    "test_x = np.reshape(test_x, [-1,28*28])\n",
    "\n",
    "train_y = to_categorical(train_y, 10) ## one-hot encoding\n",
    "test_y = to_categorical(test_y,10)\n",
    "\n",
    "# tf.data.Dataset\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y)).shuffle(buffer_size=70000).batch(batch_size)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_x, test_y)).batch(batch_size)\n",
    "\n",
    "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(256, activation=tf.nn.relu, input_shape=(784,)))  ## input_shape at first layer appended to Sequantial\n",
    "    model.add(keras.layers.Dense(10))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\jmson\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 203,530\n",
      "Trainable params: 203,530\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, x, labels):\n",
    "    logits = model(x)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels))\n",
    "    return loss\n",
    "\n",
    "def evaluate(model, x, labels):\n",
    "    logits = model(x)\n",
    "    correct_pred = tf.equal(tf.argmax(logits,1), tf.argmax(labels,1))\n",
    "    acc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    return acc\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, epsilon=0.01)\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_fn(model, X, Y)\n",
    "\n",
    "simple_optm = optimizer.minimize(loss)\n",
    "\n",
    "tr_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "grads_and_vars = optimizer.compute_gradients(loss, var_list = tr_vars)\n",
    "optm = optimizer.apply_gradients(grads_and_vars)\n",
    "\n",
    "accuracy = evaluate(model, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "num_train_data = train_x.shape[0]\n",
    "num_test_data = test_x.shape[0]\n",
    "\n",
    "train_iterator = train_dataset.make_initializable_iterator()\n",
    "test_iterator = test_dataset.make_initializable_iterator()\n",
    "\n",
    "tr_x, tr_y = train_iterator.get_next()\n",
    "ts_x, ts_y = test_iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#EPOCH: 1\n",
      "train_avg_loss:0.0018984116543530642\ttrain_avg_accuracy:1.0\n",
      "test_avg_accuracy:0.9811000072956085\n",
      "#EPOCH: 2\n",
      "train_avg_loss:0.0018738612000258096\ttrain_avg_accuracy:1.0\n",
      "#EPOCH: 3\n",
      "train_avg_loss:0.001844504673499614\ttrain_avg_accuracy:1.0\n",
      "#EPOCH: 4\n",
      "train_avg_loss:0.0018287461281336923\ttrain_avg_accuracy:1.0\n",
      "#EPOCH: 5\n",
      "train_avg_loss:0.001800619849357948\ttrain_avg_accuracy:1.0\n",
      "#EPOCH: 6\n",
      "train_avg_loss:0.0017746064980504646\ttrain_avg_accuracy:1.0\n",
      "test_avg_accuracy:0.9810000073909759\n",
      "#EPOCH: 7\n",
      "train_avg_loss:0.0017540035435498187\ttrain_avg_accuracy:1.0\n",
      "#EPOCH: 8\n",
      "train_avg_loss:0.0017300208300487915\ttrain_avg_accuracy:1.0\n",
      "#EPOCH: 9\n",
      "train_avg_loss:0.0017103379829010617\ttrain_avg_accuracy:1.0\n",
      "#EPOCH: 10\n",
      "train_avg_loss:0.0016946350112023842\ttrain_avg_accuracy:1.0\n",
      "#EPOCH: 11\n",
      "train_avg_loss:0.0016746630832494703\ttrain_avg_accuracy:1.0\n",
      "test_avg_accuracy:0.9809000068902969\n",
      "#EPOCH: 12\n",
      "train_avg_loss:0.001653754301648102\ttrain_avg_accuracy:1.0\n",
      "#EPOCH: 13\n",
      "train_avg_loss:0.0016334808240450608\ttrain_avg_accuracy:1.0\n",
      "#EPOCH: 14\n",
      "train_avg_loss:0.0016164030613920962\ttrain_avg_accuracy:1.0\n",
      "#EPOCH: 15\n",
      "train_avg_loss:0.0015884786834552264\ttrain_avg_accuracy:1.0\n",
      "#EPOCH: 16\n",
      "train_avg_loss:0.0015782034294655508\ttrain_avg_accuracy:1.0\n",
      "test_avg_accuracy:0.9812000066041946\n",
      "#EPOCH: 17\n",
      "train_avg_loss:0.0015587327983909442\ttrain_avg_accuracy:1.0\n",
      "#EPOCH: 18\n",
      "train_avg_loss:0.001538273608603049\ttrain_avg_accuracy:1.0\n",
      "#EPOCH: 19\n",
      "train_avg_loss:0.0015265197768167128\ttrain_avg_accuracy:1.0\n",
      "#EPOCH: 20\n",
      "train_avg_loss:0.0015107093202338243\ttrain_avg_accuracy:1.0\n",
      "#EPOCH: 21\n",
      "train_avg_loss:0.0014929691901003632\ttrain_avg_accuracy:1.0\n",
      "test_avg_accuracy:0.9808000075817108\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    status=checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "    status.initialize_or_restore(sess)\n",
    "\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_loss = 0\n",
    "        avg_train_acc = 0\n",
    "        avg_test_acc = 0\n",
    "        train_step = 0\n",
    "        test_step = 0\n",
    "\n",
    "        sess.run(train_iterator.initializer)\n",
    "        for _ in range(num_train_data//batch_size):\n",
    "            train_input, train_label = sess.run([tr_x, tr_y])\n",
    "            step_loss, step_accuracy, _ = sess.run([loss, accuracy, optm], feed_dict={X:train_input, Y: train_label})\n",
    "            avg_loss += step_loss\n",
    "            avg_train_acc += step_accuracy\n",
    "            train_step += 1\n",
    "        avg_loss /= train_step\n",
    "        avg_train_acc /= train_step\n",
    "        \n",
    "        print(f\"#EPOCH: {epoch+1}\")\n",
    "        print(f\"train_avg_loss:{avg_loss}\\ttrain_avg_accuracy:{avg_train_acc}\")\n",
    "        \n",
    "        if epoch%5 == 0:\n",
    "            sess.run(test_iterator.initializer)\n",
    "            for _ in range(num_test_data//batch_size):\n",
    "                test_x, test_y = sess.run([ts_x, ts_y])\n",
    "                step_accuracy = sess.run(accuracy, feed_dict={X:test_x, Y:test_y})\n",
    "                avg_test_acc += step_accuracy\n",
    "                test_step += 1 \n",
    "            avg_test_acc /= test_step\n",
    "        \n",
    "            print(f\"test_avg_accuracy:{avg_test_acc}\")\n",
    "            \n",
    "            checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functional model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_func():\n",
    "    with tf.variable_scope('functional', reuse=False):\n",
    "        inputs =  keras.Input(shape=(784,))\n",
    "        fc_layer1 = keras.layers.Dense(256, activation = tf.nn.relu)(inputs)\n",
    "        logits = keras.layers.Dense(10)(fc_layer1)\n",
    "        return keras.Model(inputs=inputs, outputs=logits)\n",
    "\n",
    "model_func = create_model_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 203,530\n",
      "Trainable params: 203,530\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_func.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_dir = os.getcwd()\n",
    "ckpt_dir_name = 'checkpoints'\n",
    "model_dir_name = 'mnist_keras_functional_session'\n",
    "\n",
    "checkpoint_dir = os.path.join(cur_dir, ckpt_dir_name, model_dir_name)\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, model_dir_name)\n",
    "\n",
    "## TensorBoard\n",
    "log_dir = os.path.join(cur_dir, 'tensorboard', model_dir_name)\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, epsilon=0.01)\n",
    "checkpoint = tf.train.Checkpoint(model_func=model_func, optimizer=optimizer)\n",
    "\n",
    "loss_func = loss_fn(model_func, X, Y)\n",
    "\n",
    "simple_optm = optimizer.minimize(loss_func)\n",
    "\n",
    "\n",
    "tr_vars_func = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='functional')\n",
    "grads_and_vars_func = optimizer.compute_gradients(loss_func, var_list = tr_vars_func)\n",
    "optm_func = optimizer.apply_gradients(grads_and_vars_func)\n",
    "\n",
    "accuracy_func = evaluate(model_func, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#EPOCH: 1\n",
      "train_avg_loss:0.006938397919099467\ttrain_avg_accuracy:0.9996666669845581\n",
      "test_avg_accuracy:0.9815000075101853\n",
      "#EPOCH: 2\n",
      "train_avg_loss:0.00672722139240553\ttrain_avg_accuracy:0.9997333335876465\n",
      "#EPOCH: 3\n",
      "train_avg_loss:0.006539068906373965\ttrain_avg_accuracy:0.9997000002861023\n",
      "#EPOCH: 4\n",
      "train_avg_loss:0.006261589742595485\ttrain_avg_accuracy:0.9997666668891907\n",
      "#EPOCH: 5\n",
      "train_avg_loss:0.0061399349711913\ttrain_avg_accuracy:0.9997333335876465\n",
      "#EPOCH: 6\n",
      "train_avg_loss:0.005965125939401333\ttrain_avg_accuracy:0.9997500002384185\n",
      "test_avg_accuracy:0.9811000066995621\n",
      "#EPOCH: 7\n",
      "train_avg_loss:0.005809882700850722\ttrain_avg_accuracy:0.9997833335399627\n",
      "#EPOCH: 8\n",
      "train_avg_loss:0.005618222850607708\ttrain_avg_accuracy:0.9997666668891907\n",
      "#EPOCH: 9\n",
      "train_avg_loss:0.00545873034648442\ttrain_avg_accuracy:0.9998166668415069\n",
      "#EPOCH: 10\n",
      "train_avg_loss:0.005342780281498562\ttrain_avg_accuracy:0.9998333334922791\n",
      "#EPOCH: 11\n",
      "train_avg_loss:0.0051355299974481265\ttrain_avg_accuracy:0.9998833334445953\n",
      "test_avg_accuracy:0.9810000073909759\n",
      "#EPOCH: 12\n",
      "train_avg_loss:0.005058024611498695\ttrain_avg_accuracy:0.9998500001430511\n",
      "#EPOCH: 13\n",
      "train_avg_loss:0.004860118005405335\ttrain_avg_accuracy:0.9998833334445953\n",
      "#EPOCH: 14\n",
      "train_avg_loss:0.0047370056922469905\ttrain_avg_accuracy:0.9999000000953674\n",
      "#EPOCH: 15\n",
      "train_avg_loss:0.004606292792304885\ttrain_avg_accuracy:0.9999166667461395\n",
      "#EPOCH: 16\n",
      "train_avg_loss:0.004541964488356219\ttrain_avg_accuracy:0.9999000000953674\n",
      "test_avg_accuracy:0.9814000082015991\n",
      "#EPOCH: 17\n",
      "train_avg_loss:0.004401799962215592\ttrain_avg_accuracy:0.9999166667461395\n",
      "#EPOCH: 18\n",
      "train_avg_loss:0.004305667419782063\ttrain_avg_accuracy:0.9999333333969116\n",
      "#EPOCH: 19\n",
      "train_avg_loss:0.0042101467123332745\ttrain_avg_accuracy:0.9999000000953674\n",
      "#EPOCH: 20\n",
      "train_avg_loss:0.004101457013845599\ttrain_avg_accuracy:0.9999500000476838\n",
      "#EPOCH: 21\n",
      "train_avg_loss:0.004019154183139714\ttrain_avg_accuracy:0.9999500000476838\n",
      "test_avg_accuracy:0.9814000082015991\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    status = checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "    status.initialize_or_restore(sess)\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_loss = 0\n",
    "        avg_train_acc = 0\n",
    "        avg_test_acc = 0\n",
    "        train_step = 0\n",
    "        test_step = 0\n",
    "\n",
    "        sess.run(train_iterator.initializer)\n",
    "        for _ in range(num_train_data//batch_size):\n",
    "            train_input, train_label = sess.run([tr_x, tr_y])\n",
    "            step_loss, step_accuracy, _ = sess.run([loss_func, accuracy_func, optm_func], feed_dict={X:train_input, Y: train_label})\n",
    "            avg_loss += step_loss\n",
    "            avg_train_acc += step_accuracy\n",
    "            train_step += 1\n",
    "        avg_loss /= train_step\n",
    "        avg_train_acc /= train_step\n",
    "        \n",
    "        print(f\"#EPOCH: {epoch+1}\")\n",
    "        print(f\"train_avg_loss:{avg_loss}\\ttrain_avg_accuracy:{avg_train_acc}\")\n",
    "        \n",
    "        if epoch%5 == 0:\n",
    "            sess.run(test_iterator.initializer)\n",
    "            for _ in range(num_test_data//batch_size):\n",
    "                test_x, test_y = sess.run([ts_x, ts_y])\n",
    "                step_accuracy = sess.run(accuracy_func, feed_dict={X:test_x, Y:test_y})\n",
    "                avg_test_acc += step_accuracy\n",
    "                test_step += 1 \n",
    "            avg_test_acc /= test_step\n",
    "        \n",
    "            print(f\"test_avg_accuracy:{avg_test_acc}\")\n",
    "            \n",
    "            checkpoint.save(file_prefix=checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassModel(keras.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        with tf.variable_scope('class_based'):\n",
    "            self.fc_layer1 = keras.layers.Dense(256, activation='relu')\n",
    "            self.logits = keras.layers.Dense(10)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        net = self.fc_layer1(inputs)\n",
    "        net = self.logits(net)\n",
    "        return net\n",
    "        \n",
    "model_class = ClassModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_dir = os.getcwd()\n",
    "ckpt_dir_name = 'checkpoints'\n",
    "model_dir_name = 'mnist_keras_class_session'\n",
    "\n",
    "checkpoint_dir = os.path.join(cur_dir, ckpt_dir_name, model_dir_name)\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, model_dir_name)\n",
    "\n",
    "## TensorBoard\n",
    "log_dir = os.path.join(cur_dir, 'tensorboard', model_dir_name)\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No variables to optimize.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-608493c5e664>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mtr_vars_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAINABLE_VARIABLES\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'class_based'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mgrads_and_vars_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtr_vars_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0moptm_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads_and_vars_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jmson\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\u001b[0m in \u001b[0;36mcompute_gradients\u001b[1;34m(self, loss, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, grad_loss)\u001b[0m\n\u001b[0;32m    504\u001b[0m     \u001b[0mprocessors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_get_processor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 506\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No variables to optimize.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    507\u001b[0m     \u001b[0mvar_refs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprocessors\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m     grads = gradients.gradients(\n",
      "\u001b[1;31mValueError\u001b[0m: No variables to optimize."
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, epsilon=0.01)\n",
    "checkpoint = tf.train.Checkpoint(model_func=model_class, optimizer=optimizer)\n",
    "\n",
    "loss_class = loss_fn(model_class, X, Y)\n",
    "\n",
    "simple_optm = optimizer.minimize(loss_class)\n",
    "\n",
    "\n",
    "tr_vars_class = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='class_based')\n",
    "grads_and_vars_class = optimizer.compute_gradients(loss_func, var_list = tr_vars_class)\n",
    "optm_func = optimizer.apply_gradients(grads_and_vars_class)\n",
    "\n",
    "accuracy_func = evaluate(model_class, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
