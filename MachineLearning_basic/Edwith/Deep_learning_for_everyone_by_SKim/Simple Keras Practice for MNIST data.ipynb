{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras on Session mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "#import tensorflow.contrib.eager as tfe\n",
    "#tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "training_epochs = 21\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_dir = os.getcwd()\n",
    "ckpt_dir_name = 'checkpoints'\n",
    "model_dir_name = 'mnist_keras_session'\n",
    "\n",
    "checkpoint_dir = os.path.join(cur_dir, ckpt_dir_name, model_dir_name)\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, model_dir_name)\n",
    "\n",
    "## TensorBoard\n",
    "log_dir = os.path.join(cur_dir, 'tensorboard', model_dir_name)\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,) (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "mnist = keras.datasets.mnist\n",
    "\n",
    "(train_x, train_y), (test_x, test_y) = mnist.load_data()\n",
    "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000, 10) (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "train_x = train_x.astype(np.float32) / 255\n",
    "test_x = test_x.astype(np.float32) / 255\n",
    "\n",
    "train_x = np.reshape(train_x, [-1,28*28])\n",
    "test_x = np.reshape(test_x, [-1,28*28])\n",
    "\n",
    "train_y = to_categorical(train_y, 10) ## one-hot encoding\n",
    "test_y = to_categorical(test_y,10)\n",
    "\n",
    "# tf.data.Dataset\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y)).shuffle(buffer_size=70000).batch(batch_size)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_x, test_y)).batch(batch_size)\n",
    "\n",
    "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(256, activation=tf.nn.relu, input_shape=(784,)))  ## input_shape at first layer appended to Sequantial\n",
    "    model.add(keras.layers.Dense(10))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, x, labels):\n",
    "    logits = model(x)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels))\n",
    "    return loss\n",
    "\n",
    "def evaluate(model, x, labels):\n",
    "    logits = model(x)\n",
    "    correct_pred = tf.equal(tf.argmax(logits,1), tf.argmax(labels,1))\n",
    "    acc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    return acc\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, epsilon=0.01)\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_fn(model, X, Y)\n",
    "\n",
    "simple_optm = optimizer.minimize(loss)\n",
    "\n",
    "tr_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "grads_and_vars = optimizer.compute_gradients(loss, var_list = tr_vars)\n",
    "optm = optimizer.apply_gradients(grads_and_vars)\n",
    "\n",
    "accuracy = evaluate(model, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "num_train_data = train_x.shape[0]\n",
    "num_test_data = test_x.shape[0]\n",
    "\n",
    "train_iterator = train_dataset.make_initializable_iterator()\n",
    "test_iterator = test_dataset.make_initializable_iterator()\n",
    "\n",
    "tr_x, tr_y = train_iterator.get_next()\n",
    "ts_x, ts_y = test_iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    status=checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "    status.initialize_or_restore(sess)\n",
    "\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_loss = 0\n",
    "        avg_train_acc = 0\n",
    "        avg_test_acc = 0\n",
    "        train_step = 0\n",
    "        test_step = 0\n",
    "\n",
    "        sess.run(train_iterator.initializer)\n",
    "        for _ in range(num_train_data//batch_size):\n",
    "            train_input, train_label = sess.run([tr_x, tr_y])\n",
    "            step_loss, step_accuracy, _ = sess.run([loss, accuracy, optm], feed_dict={X:train_input, Y: train_label})\n",
    "            avg_loss += step_loss\n",
    "            avg_train_acc += step_accuracy\n",
    "            train_step += 1\n",
    "        avg_loss /= train_step\n",
    "        avg_train_acc /= train_step\n",
    "        \n",
    "        print(f\"#EPOCH: {epoch+1}\")\n",
    "        print(f\"train_avg_loss:{avg_loss}\\ttrain_avg_accuracy:{avg_train_acc}\")\n",
    "        \n",
    "        if epoch%5 == 0:\n",
    "            sess.run(test_iterator.initializer)\n",
    "            for _ in range(num_test_data//batch_size):\n",
    "                test_x, test_y = sess.run([ts_x, ts_y])\n",
    "                step_accuracy = sess.run(accuracy, feed_dict={X:test_x, Y:test_y})\n",
    "                avg_test_acc += step_accuracy\n",
    "                test_step += 1 \n",
    "            avg_test_acc /= test_step\n",
    "        \n",
    "            print(f\"test_avg_accuracy:{avg_test_acc}\")\n",
    "            \n",
    "            checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functional model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_func():\n",
    "    with tf.variable_scope('functional', reuse=False):\n",
    "        inputs =  keras.Input(shape=(784,))\n",
    "        fc_layer1 = keras.layers.Dense(256, activation = tf.nn.relu)(inputs)\n",
    "        logits = keras.layers.Dense(10)(fc_layer1)\n",
    "        return keras.Model(inputs=inputs, outputs=logits)\n",
    "\n",
    "model_func = create_model_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_func.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_dir = os.getcwd()\n",
    "ckpt_dir_name = 'checkpoints'\n",
    "model_dir_name = 'mnist_keras_functional_session'\n",
    "\n",
    "checkpoint_dir = os.path.join(cur_dir, ckpt_dir_name, model_dir_name)\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, model_dir_name)\n",
    "\n",
    "## TensorBoard\n",
    "log_dir = os.path.join(cur_dir, 'tensorboard', model_dir_name)\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, epsilon=0.01)\n",
    "checkpoint = tf.train.Checkpoint(model_func=model_func, optimizer=optimizer)\n",
    "\n",
    "loss_func = loss_fn(model_func, X, Y)\n",
    "\n",
    "simple_optm = optimizer.minimize(loss_func)\n",
    "\n",
    "\n",
    "tr_vars_func = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='functional')\n",
    "grads_and_vars_func = optimizer.compute_gradients(loss_func, var_list = tr_vars_func)\n",
    "optm_func = optimizer.apply_gradients(grads_and_vars_func)\n",
    "\n",
    "accuracy_func = evaluate(model_func, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    status = checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "    status.initialize_or_restore(sess)\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_loss = 0\n",
    "        avg_train_acc = 0\n",
    "        avg_test_acc = 0\n",
    "        train_step = 0\n",
    "        test_step = 0\n",
    "\n",
    "        sess.run(train_iterator.initializer)\n",
    "        for _ in range(num_train_data//batch_size):\n",
    "            train_input, train_label = sess.run([tr_x, tr_y])\n",
    "            step_loss, step_accuracy, _ = sess.run([loss_func, accuracy_func, optm_func], feed_dict={X:train_input, Y: train_label})\n",
    "            avg_loss += step_loss\n",
    "            avg_train_acc += step_accuracy\n",
    "            train_step += 1\n",
    "        avg_loss /= train_step\n",
    "        avg_train_acc /= train_step\n",
    "        \n",
    "        print(f\"#EPOCH: {epoch+1}\")\n",
    "        print(f\"train_avg_loss:{avg_loss}\\ttrain_avg_accuracy:{avg_train_acc}\")\n",
    "        \n",
    "        if epoch%5 == 0:\n",
    "            sess.run(test_iterator.initializer)\n",
    "            for _ in range(num_test_data//batch_size):\n",
    "                test_x, test_y = sess.run([ts_x, ts_y])\n",
    "                step_accuracy = sess.run(accuracy_func, feed_dict={X:test_x, Y:test_y})\n",
    "                avg_test_acc += step_accuracy\n",
    "                test_step += 1 \n",
    "            avg_test_acc /= test_step\n",
    "        \n",
    "            print(f\"test_avg_accuracy:{avg_test_acc}\")\n",
    "            \n",
    "            checkpoint.save(file_prefix=checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassModel(keras.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.fc_layer1 = keras.layers.Dense(256, activation='relu')\n",
    "        self.logits = keras.layers.Dense(10)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        net = self.fc_layer1(inputs)\n",
    "        net = self.logits(net)\n",
    "        return net\n",
    "        \n",
    "with tf.variable_scope('class_based',use_resource=True):\n",
    "    model_class = ClassModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_dir = os.getcwd()\n",
    "ckpt_dir_name = 'checkpoints'\n",
    "model_dir_name = 'mnist_keras_class_session'\n",
    "\n",
    "checkpoint_dir = os.path.join(cur_dir, ckpt_dir_name, model_dir_name)\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, model_dir_name)\n",
    "\n",
    "## TensorBoard\n",
    "log_dir = os.path.join(cur_dir, 'tensorboard', model_dir_name)\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, epsilon=0.01)\n",
    "checkpoint = tf.train.Checkpoint(model_class=model_class, optimizer=optimizer)\n",
    "\n",
    "loss_class = loss_fn(model_class, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                multiple                  200960    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  2570      \n",
      "=================================================================\n",
      "Total params: 203,530\n",
      "Trainable params: 203,530\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_class.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_optm = optimizer.minimize(loss_class)\n",
    "\n",
    "\n",
    "tr_vars_class = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='class_model')\n",
    "grads_and_vars_class = optimizer.compute_gradients(loss_class, var_list = tr_vars_class)\n",
    "optm_class = optimizer.apply_gradients(grads_and_vars_class)\n",
    "\n",
    "accuracy_class = evaluate(model_class, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'class_model/dense/kernel:0' shape=(784, 256) dtype=float32>,\n",
       " <tf.Variable 'class_model/dense/bias:0' shape=(256,) dtype=float32>,\n",
       " <tf.Variable 'class_model/dense_1/kernel:0' shape=(256, 10) dtype=float32>,\n",
       " <tf.Variable 'class_model/dense_1/bias:0' shape=(10,) dtype=float32>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_vars_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.variable_scope does not work for class-based keras model!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#EPOCH: 1\n",
      "train_avg_loss:0.04138871586571137\ttrain_avg_accuracy:0.989616675277551\n",
      "test_avg_accuracy:0.979500008225441\n",
      "#EPOCH: 2\n",
      "train_avg_loss:0.0391157097571219\ttrain_avg_accuracy:0.9905000087618828\n",
      "#EPOCH: 3\n",
      "train_avg_loss:0.03695347749395296\ttrain_avg_accuracy:0.9911833412448565\n",
      "#EPOCH: 4\n",
      "train_avg_loss:0.03533846121281385\ttrain_avg_accuracy:0.9914666741093\n",
      "#EPOCH: 5\n",
      "train_avg_loss:0.03340715296411266\ttrain_avg_accuracy:0.9921833402911822\n",
      "#EPOCH: 6\n",
      "train_avg_loss:0.031729518667949985\ttrain_avg_accuracy:0.9929333397746086\n",
      "test_avg_accuracy:0.9800000065565109\n",
      "#EPOCH: 7\n",
      "train_avg_loss:0.029954073789995164\ttrain_avg_accuracy:0.9936166724562645\n",
      "#EPOCH: 8\n",
      "train_avg_loss:0.028584290660219266\ttrain_avg_accuracy:0.9937666725118955\n",
      "#EPOCH: 9\n",
      "train_avg_loss:0.02721952805101561\ttrain_avg_accuracy:0.9943000052372615\n",
      "#EPOCH: 10\n",
      "train_avg_loss:0.02583166576611499\ttrain_avg_accuracy:0.9945333384474119\n",
      "#EPOCH: 11\n",
      "train_avg_loss:0.024644941174968456\ttrain_avg_accuracy:0.9951166712244351\n",
      "test_avg_accuracy:0.9808000099658966\n",
      "#EPOCH: 12\n",
      "train_avg_loss:0.023521942036847272\ttrain_avg_accuracy:0.995250004529953\n",
      "#EPOCH: 13\n",
      "train_avg_loss:0.022284200116991996\ttrain_avg_accuracy:0.9957166706522306\n",
      "#EPOCH: 14\n",
      "train_avg_loss:0.021310202993530158\ttrain_avg_accuracy:0.9960833368698756\n",
      "#EPOCH: 15\n",
      "train_avg_loss:0.020302374023012815\ttrain_avg_accuracy:0.9962833368778229\n",
      "#EPOCH: 16\n",
      "train_avg_loss:0.019485129107100266\ttrain_avg_accuracy:0.9967666696508726\n",
      "test_avg_accuracy:0.9812000060081482\n",
      "#EPOCH: 17\n",
      "train_avg_loss:0.018661380402433377\ttrain_avg_accuracy:0.9970833361148834\n",
      "#EPOCH: 18\n",
      "train_avg_loss:0.017827150132119034\ttrain_avg_accuracy:0.9971166694164276\n",
      "#EPOCH: 19\n",
      "train_avg_loss:0.017057207591909294\ttrain_avg_accuracy:0.9973000025749207\n",
      "#EPOCH: 20\n",
      "train_avg_loss:0.0163024646412426\ttrain_avg_accuracy:0.9976666688919067\n",
      "#EPOCH: 21\n",
      "train_avg_loss:0.01560301628235417\ttrain_avg_accuracy:0.9978333353996277\n",
      "test_avg_accuracy:0.9812000060081482\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    status=checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "    status.initialize_or_restore(sess)\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_loss = 0\n",
    "        avg_train_acc = 0\n",
    "        avg_test_acc = 0\n",
    "        train_step = 0\n",
    "        test_step = 0\n",
    "\n",
    "        sess.run(train_iterator.initializer)\n",
    "        for _ in range(num_train_data//batch_size):\n",
    "            train_input, train_label = sess.run([tr_x, tr_y])\n",
    "            step_loss, step_accuracy, _ = sess.run([loss_class, accuracy_class, optm_class], feed_dict={X:train_input, Y: train_label})\n",
    "            avg_loss += step_loss\n",
    "            avg_train_acc += step_accuracy\n",
    "            train_step += 1\n",
    "        avg_loss /= train_step\n",
    "        avg_train_acc /= train_step\n",
    "        \n",
    "        print(f\"#EPOCH: {epoch+1}\")\n",
    "        print(f\"train_avg_loss:{avg_loss}\\ttrain_avg_accuracy:{avg_train_acc}\")\n",
    "        \n",
    "        if epoch%5 == 0:\n",
    "            sess.run(test_iterator.initializer)\n",
    "            for _ in range(num_test_data//batch_size):\n",
    "                test_x, test_y = sess.run([ts_x, ts_y])\n",
    "                step_accuracy = sess.run(accuracy_class, feed_dict={X:test_x, Y:test_y})\n",
    "                avg_test_acc += step_accuracy\n",
    "                test_step += 1 \n",
    "            avg_test_acc /= test_step\n",
    "        \n",
    "            print(f\"test_avg_accuracy:{avg_test_acc}\")\n",
    "            \n",
    "            checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Layer\n",
    "- inherit keras.layers.Layer\n",
    "- implement following methods\n",
    "1. build (add_weight)\n",
    "2. call\n",
    "3. compute_output_shape\n",
    "4. get_config (optionally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
