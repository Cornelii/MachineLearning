{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras on Session mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "#import tensorflow.contrib.eager as tfe\n",
    "#tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "training_epochs = 21\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_dir = os.getcwd()\n",
    "ckpt_dir_name = 'checkpoints'\n",
    "model_dir_name = 'mnist_keras_session'\n",
    "\n",
    "checkpoint_dir = os.path.join(cur_dir, ckpt_dir_name, model_dir_name)\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, model_dir_name)\n",
    "\n",
    "## TensorBoard\n",
    "log_dir = os.path.join(cur_dir, 'tensorboard', model_dir_name)\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,) (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "mnist = keras.datasets.mnist\n",
    "\n",
    "(train_x, train_y), (test_x, test_y) = mnist.load_data()\n",
    "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000, 10) (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "train_x = train_x.astype(np.float32) / 255\n",
    "test_x = test_x.astype(np.float32) / 255\n",
    "\n",
    "train_x = np.reshape(train_x, [-1,28*28])\n",
    "test_x = np.reshape(test_x, [-1,28*28])\n",
    "\n",
    "train_y = to_categorical(train_y, 10) ## one-hot encoding\n",
    "test_y = to_categorical(test_y,10)\n",
    "\n",
    "# tf.data.Dataset\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y)).shuffle(buffer_size=70000).batch(batch_size)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_x, test_y)).batch(batch_size)\n",
    "\n",
    "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(256, activation=tf.nn.relu, input_shape=(784,)))  ## input_shape at first layer appended to Sequantial\n",
    "    model.add(keras.layers.Dense(10))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 203,530\n",
      "Trainable params: 203,530\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, x, labels):\n",
    "    logits = model(x)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels))\n",
    "    return loss\n",
    "\n",
    "def evaluate(model, x, labels):\n",
    "    logits = model(x)\n",
    "    correct_pred = tf.equal(tf.argmax(logits,1), tf.argmax(labels,1))\n",
    "    acc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    return acc\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, epsilon=0.01)\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_fn(model, X, Y)\n",
    "\n",
    "simple_optm = optimizer.minimize(loss)\n",
    "\n",
    "tr_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "grads_and_vars = optimizer.compute_gradients(loss, var_list = tr_vars)\n",
    "optm = optimizer.apply_gradients(grads_and_vars)\n",
    "\n",
    "accuracy = evaluate(model, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "num_train_data = train_x.shape[0]\n",
    "num_test_data = test_x.shape[0]\n",
    "\n",
    "train_iterator = train_dataset.make_initializable_iterator()\n",
    "test_iterator = test_dataset.make_initializable_iterator()\n",
    "\n",
    "tr_x, tr_y = train_iterator.get_next()\n",
    "ts_x, ts_y = test_iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-4ee91c5d3bac>, line 24)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-13-4ee91c5d3bac>\"\u001b[1;36m, line \u001b[1;32m24\u001b[0m\n\u001b[1;33m    print(f\"#EPOCH: {epoch+1}\")\u001b[0m\n\u001b[1;37m                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    status=checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "    status.initialize_or_restore(sess)\n",
    "\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_loss = 0\n",
    "        avg_train_acc = 0\n",
    "        avg_test_acc = 0\n",
    "        train_step = 0\n",
    "        test_step = 0\n",
    "\n",
    "        sess.run(train_iterator.initializer)\n",
    "        for _ in range(num_train_data//batch_size):\n",
    "            train_input, train_label = sess.run([tr_x, tr_y])\n",
    "            step_loss, step_accuracy, _ = sess.run([loss, accuracy, optm], feed_dict={X:train_input, Y: train_label})\n",
    "            avg_loss += step_loss\n",
    "            avg_train_acc += step_accuracy\n",
    "            train_step += 1\n",
    "        avg_loss /= train_step\n",
    "        avg_train_acc /= train_step\n",
    "        \n",
    "        print(f\"#EPOCH: {epoch+1}\")\n",
    "        print(f\"train_avg_loss:{avg_loss}\\ttrain_avg_accuracy:{avg_train_acc}\")\n",
    "        \n",
    "        if epoch%5 == 0:\n",
    "            sess.run(test_iterator.initializer)\n",
    "            for _ in range(num_test_data//batch_size):\n",
    "                test_x, test_y = sess.run([ts_x, ts_y])\n",
    "                step_accuracy = sess.run(accuracy, feed_dict={X:test_x, Y:test_y})\n",
    "                avg_test_acc += step_accuracy\n",
    "                test_step += 1 \n",
    "            avg_test_acc /= test_step\n",
    "        \n",
    "            print(f\"test_avg_accuracy:{avg_test_acc}\")\n",
    "            \n",
    "            checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functional model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_func():\n",
    "    with tf.variable_scope('functional', reuse=False):\n",
    "        inputs =  keras.Input(shape=(784,))\n",
    "        fc_layer1 = keras.layers.Dense(256, activation = tf.nn.relu)(inputs)\n",
    "        logits = keras.layers.Dense(10)(fc_layer1)\n",
    "        return keras.Model(inputs=inputs, outputs=logits)\n",
    "\n",
    "model_func = create_model_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_func.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_dir = os.getcwd()\n",
    "ckpt_dir_name = 'checkpoints'\n",
    "model_dir_name = 'mnist_keras_functional_session'\n",
    "\n",
    "checkpoint_dir = os.path.join(cur_dir, ckpt_dir_name, model_dir_name)\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, model_dir_name)\n",
    "\n",
    "## TensorBoard\n",
    "log_dir = os.path.join(cur_dir, 'tensorboard', model_dir_name)\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, epsilon=0.01)\n",
    "checkpoint = tf.train.Checkpoint(model_func=model_func, optimizer=optimizer)\n",
    "\n",
    "loss_func = loss_fn(model_func, X, Y)\n",
    "\n",
    "simple_optm = optimizer.minimize(loss_func)\n",
    "\n",
    "\n",
    "tr_vars_func = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='functional')\n",
    "grads_and_vars_func = optimizer.compute_gradients(loss_func, var_list = tr_vars_func)\n",
    "optm_func = optimizer.apply_gradients(grads_and_vars_func)\n",
    "\n",
    "accuracy_func = evaluate(model_func, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    status = checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "    status.initialize_or_restore(sess)\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_loss = 0\n",
    "        avg_train_acc = 0\n",
    "        avg_test_acc = 0\n",
    "        train_step = 0\n",
    "        test_step = 0\n",
    "\n",
    "        sess.run(train_iterator.initializer)\n",
    "        for _ in range(num_train_data//batch_size):\n",
    "            train_input, train_label = sess.run([tr_x, tr_y])\n",
    "            step_loss, step_accuracy, _ = sess.run([loss_func, accuracy_func, optm_func], feed_dict={X:train_input, Y: train_label})\n",
    "            avg_loss += step_loss\n",
    "            avg_train_acc += step_accuracy\n",
    "            train_step += 1\n",
    "        avg_loss /= train_step\n",
    "        avg_train_acc /= train_step\n",
    "        \n",
    "        print(f\"#EPOCH: {epoch+1}\")\n",
    "        print(f\"train_avg_loss:{avg_loss}\\ttrain_avg_accuracy:{avg_train_acc}\")\n",
    "        \n",
    "        if epoch%5 == 0:\n",
    "            sess.run(test_iterator.initializer)\n",
    "            for _ in range(num_test_data//batch_size):\n",
    "                test_x, test_y = sess.run([ts_x, ts_y])\n",
    "                step_accuracy = sess.run(accuracy_func, feed_dict={X:test_x, Y:test_y})\n",
    "                avg_test_acc += step_accuracy\n",
    "                test_step += 1 \n",
    "            avg_test_acc /= test_step\n",
    "        \n",
    "            print(f\"test_avg_accuracy:{avg_test_acc}\")\n",
    "            \n",
    "            checkpoint.save(file_prefix=checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassModel(keras.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.fc_layer1 = keras.layers.Dense(256, activation='relu')\n",
    "        self.logits = keras.layers.Dense(10)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        net = self.fc_layer1(inputs)\n",
    "        net = self.logits(net)\n",
    "        return net\n",
    "        \n",
    "with tf.variable_scope('class_based',use_resource=True):\n",
    "    model_class = ClassModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_dir = os.getcwd()\n",
    "ckpt_dir_name = 'checkpoints'\n",
    "model_dir_name = 'mnist_keras_class_session'\n",
    "\n",
    "checkpoint_dir = os.path.join(cur_dir, ckpt_dir_name, model_dir_name)\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, model_dir_name)\n",
    "\n",
    "## TensorBoard\n",
    "log_dir = os.path.join(cur_dir, 'tensorboard', model_dir_name)\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, epsilon=0.01)\n",
    "checkpoint = tf.train.Checkpoint(model_class=model_class, optimizer=optimizer)\n",
    "\n",
    "loss_class = loss_fn(model_class, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_optm = optimizer.minimize(loss_class)\n",
    "\n",
    "\n",
    "tr_vars_class = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='class_model')\n",
    "grads_and_vars_class = optimizer.compute_gradients(loss_class, var_list = tr_vars_class)\n",
    "optm_class = optimizer.apply_gradients(grads_and_vars_class)\n",
    "\n",
    "accuracy_class = evaluate(model_class, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_vars_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.variable_scope does not work for class-based keras model!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    status=checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "    status.initialize_or_restore(sess)\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_loss = 0\n",
    "        avg_train_acc = 0\n",
    "        avg_test_acc = 0\n",
    "        train_step = 0\n",
    "        test_step = 0\n",
    "\n",
    "        sess.run(train_iterator.initializer)\n",
    "        for _ in range(num_train_data//batch_size):\n",
    "            train_input, train_label = sess.run([tr_x, tr_y])\n",
    "            step_loss, step_accuracy, _ = sess.run([loss_class, accuracy_class, optm_class], feed_dict={X:train_input, Y: train_label})\n",
    "            avg_loss += step_loss\n",
    "            avg_train_acc += step_accuracy\n",
    "            train_step += 1\n",
    "        avg_loss /= train_step\n",
    "        avg_train_acc /= train_step\n",
    "        \n",
    "        print(f\"#EPOCH: {epoch+1}\")\n",
    "        print(f\"train_avg_loss:{avg_loss}\\ttrain_avg_accuracy:{avg_train_acc}\")\n",
    "        \n",
    "        if epoch%5 == 0:\n",
    "            sess.run(test_iterator.initializer)\n",
    "            for _ in range(num_test_data//batch_size):\n",
    "                test_x, test_y = sess.run([ts_x, ts_y])\n",
    "                step_accuracy = sess.run(accuracy_class, feed_dict={X:test_x, Y:test_y})\n",
    "                avg_test_acc += step_accuracy\n",
    "                test_step += 1 \n",
    "            avg_test_acc /= test_step\n",
    "        \n",
    "            print(f\"test_avg_accuracy:{avg_test_acc}\")\n",
    "            \n",
    "            checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Layer\n",
    "- inherit keras.layers.Layer\n",
    "- implement following methods\n",
    "1. build (add_weight)\n",
    "2. call\n",
    "3. compute_output_shape\n",
    "4. get_config (optionally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
