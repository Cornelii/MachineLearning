{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "training_epochs = 6\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_dir = os.getcwd()\n",
    "model_dir_name = 'gan_eager_mode'\n",
    "checkpoint_dir = os.path.join(cur_dir, 'checkpoints', model_dir_name)\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, model_dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,) (10000, 28, 28) (10000,)\n",
      "After preprocessing\n",
      "(60000, 28, 28, 1) (60000, 10) (10000, 28, 28, 1) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "mnist = keras.datasets.mnist\n",
    "(train_x, train_y), (test_x, test_y) = mnist.load_data()\n",
    "\n",
    "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)\n",
    "\n",
    "train_x = train_x.astype(np.float32) / 255\n",
    "test_x = test_x.astype(np.float32) / 255\n",
    "\n",
    "train_x = np.expand_dims(train_x, 3)\n",
    "test_x = np.expand_dims(test_x, 3)\n",
    "\n",
    "train_y = to_categorical(train_y, 10)\n",
    "test_y = to_categorical(test_y, 10)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y)).shuffle(buffer_size=70000).batch(batch_size)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_x, test_y)).batch(batch_size)\n",
    "\n",
    "print('After preprocessing')\n",
    "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class G(keras.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dense1 = keras.layers.Dense(49*3, activation=None)\n",
    "        self.batch1 = keras.layers.BatchNormalization()\n",
    "        self.dropout1 = keras.layers.Dropout(0.3)\n",
    "        self.relu1 = keras.layers.ReLU()\n",
    "        self.reshape1 = keras.layers.Reshape((7, 7, 3))\n",
    "        \n",
    "#         self.upsample1 = keras.layers.UpSampling2D((2, 2))\n",
    "        self.conv2_transpose = keras.layers.Conv2DTranspose(16, 3, 2, padding='SAME', activation=None)\n",
    "        self.conv2 = keras.layers.Conv2D(16, 3, padding='SAME')\n",
    "        self.batch2 = keras.layers.BatchNormalization()\n",
    "        self.relu2 = keras.layers.ReLU()\n",
    "        \n",
    "#         self.upsample2 = keras.layers.UpSampling2D((2, 2))\n",
    "        self.conv3_transpose = keras.layers.Conv2DTranspose(32, 3, 2, padding='SAME', activation=None)\n",
    "        self.conv3 = keras.layers.Conv2D(32, 3, padding='SAME')\n",
    "        self.batch3 = keras.layers.BatchNormalization()\n",
    "        self.relu3 = keras.layers.ReLU()\n",
    "        \n",
    "        self.conv4 = keras.layers.Conv2D(1, 3, padding='SAME', activation=tf.nn.sigmoid)\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        net = self.dense1(inputs)\n",
    "        net = self.batch1(net)\n",
    "        net = self.dropout1(net)\n",
    "        net = self.relu1(net)\n",
    "        net = self.reshape1(net)\n",
    "        net = self.conv2_transpose(net)\n",
    "        net = self.conv2(net)\n",
    "        net = self.batch2(net)\n",
    "        net = self.relu2(net)\n",
    "        net = self.conv3_transpose(net)\n",
    "        net = self.conv3(net)\n",
    "        net = self.batch3(net)\n",
    "        net = self.relu3(net)\n",
    "        net = self.conv4(net)\n",
    "        \n",
    "        return net\n",
    "\n",
    "        \n",
    "class D(keras.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.conv1 = keras.layers.Conv2D(16, 3, padding='SAME', activation=None)\n",
    "        # (None, 28, 28, 1) => (None, 28, 28, 16)\n",
    "        self.batch1 = keras.layers.BatchNormalization()\n",
    "        self.dropout1 = keras.layers.Dropout(0.3)\n",
    "        self.lrelu1 = keras.layers.LeakyReLU()\n",
    "        self.pool1 = keras.layers.MaxPool2D((2,2), padding='SAME')\n",
    "        # (None, 28, 28, 16) => (None, 14, 14, 16)\n",
    "        \n",
    "        self.conv2 = keras.layers.Conv2D(32, 3, padding='SAME', activation=None)\n",
    "        # (None, 14, 14, 16) => (None, 14, 14, 32)\n",
    "        self.batch2 = keras.layers.BatchNormalization()\n",
    "        self.dropout2= keras.layers.Dropout(0.3)\n",
    "        self.lrelu2 = keras.layers.LeakyReLU()\n",
    "        self.pool2 = keras.layers.MaxPool2D((2, 2), padding='SAME')\n",
    "        # (None, 14, 14, 32) => (None, 7, 7, 32)\n",
    "        \n",
    "        self.conv3 = keras.layers.Conv2D(64, 7, padding='SAME', activation=None)\n",
    "        # (None, 7, 7, 32) => (None, 1, 1, 64)\n",
    "        self.lrelu3 = keras.layers.LeakyReLU()\n",
    "        self.flatten3 = keras.layers.Flatten()\n",
    "        self.out_layer = keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "        \n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        net = self.conv1(inputs)\n",
    "        net = self.batch1(net)\n",
    "        net = self.dropout1(net)\n",
    "        net = self.lrelu1(net)\n",
    "        net = self.pool1(net)\n",
    "        \n",
    "        net = self.conv2(net)\n",
    "        net = self.batch2(net)\n",
    "        net = self.dropout2(net)\n",
    "        net = self.lrelu2(net)\n",
    "        net = self.pool2(net)\n",
    "        \n",
    "        net = self.conv3(net)\n",
    "        net = self.lrelu3(net)\n",
    "        net = self.flatten3(net)\n",
    "        net = self.out_layer(net)\n",
    "        \n",
    "        return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = G()\n",
    "dis = D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-7\n",
    "\n",
    "def g_loss_fn(g_model, d_model, z):\n",
    "    fake = d_model(g_model(z))\n",
    "    loss = -tf.reduce_mean(tf.log(fake+epsilon))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def d_loss_fn(g_model, d_model, z, x):\n",
    "    fake = d_model(g_model(z))\n",
    "    real = d_model(x)\n",
    "    loss = -tf.reduce_mean(tf.log(real+epsilon)+tf.log(1-fake+epsilon))\n",
    "    return loss\n",
    "    \n",
    "\n",
    "def g_grads(g_model, d_model, z):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = g_loss_fn(g_model, d_model, z)\n",
    "    return tape.gradient(loss, g_model.variables)\n",
    "            \n",
    "\n",
    "def d_grads(g_model, d_model, z, x):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = d_loss_fn(g_model, d_model, z, x)\n",
    "    return tape.gradient(loss, d_model.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, epsilon=0.01)\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(g=gen, d=dis, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:532: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Could not find valid device for node.\nNode: {{node FusedBatchNorm}}\nAll kernels registered for op FusedBatchNorm :\n  device='CPU'; T in [DT_FLOAT]\n [Op:FusedBatchNorm]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-1564720080e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;31m# d update\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0md_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md_grads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_grad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0md_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md_loss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-a3ef5430ffc0>\u001b[0m in \u001b[0;36md_grads\u001b[1;34m(g_model, d_model, z, x)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0md_grads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md_loss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-a3ef5430ffc0>\u001b[0m in \u001b[0;36md_loss_fn\u001b[1;34m(g_model, d_model, z, x)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0md_loss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mfake\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mreal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mfake\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    590\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m         \u001b[1;31m# Eager execution on data tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 592\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    593\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-ef720c0effff>\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, training)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2_transpose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv3_transpose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    590\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m         \u001b[1;31m# Eager execution on data tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 592\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    593\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\normalization.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, training)\u001b[0m\n\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfused\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fused_batch_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvirtual_batch_size\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m         \u001b[1;31m# Currently never reaches here since fused_batch_norm does not support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\normalization.py\u001b[0m in \u001b[0;36m_fused_batch_norm\u001b[1;34m(self, inputs, training)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     output, mean, variance = tf_utils.smart_cond(\n\u001b[1;32m--> 457\u001b[1;33m         training, _fused_batch_norm_training, _fused_batch_norm_inference)\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_bessels_correction_test_only\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[1;31m# Remove Bessel's correction to be consistent with non-fused batch norm.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36msmart_cond\u001b[1;34m(pred, true_fn, false_fn, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m         pred, true_fn=true_fn, false_fn=false_fn, name=name)\n\u001b[0;32m     53\u001b[0m   return smart_module.smart_cond(\n\u001b[1;32m---> 54\u001b[1;33m       pred, true_fn=true_fn, false_fn=false_fn, name=name)\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\smart_cond.py\u001b[0m in \u001b[0;36msmart_cond\u001b[1;34m(pred, true_fn, false_fn, name)\u001b[0m\n\u001b[0;32m     54\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mtrue_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfalse_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     return control_flow_ops.cond(pred, true_fn=true_fn, false_fn=false_fn,\n",
      "\u001b[1;32mc:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\normalization.py\u001b[0m in \u001b[0;36m_fused_batch_norm_inference\u001b[1;34m()\u001b[0m\n\u001b[0;32m    452\u001b[0m           \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m           \u001b[0mis_training\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m           data_format=self._data_format)\n\u001b[0m\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     output, mean, variance = tf_utils.smart_cond(\n",
      "\u001b[1;32mc:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py\u001b[0m in \u001b[0;36mfused_batch_norm\u001b[1;34m(x, scale, offset, mean, variance, epsilon, data_format, is_training, name)\u001b[0m\n\u001b[0;32m   1180\u001b[0m       \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1181\u001b[0m       \u001b[0mis_training\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_training\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1182\u001b[1;33m       name=name)\n\u001b[0m\u001b[0;32m   1183\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_var\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36m_fused_batch_norm\u001b[1;34m(x, scale, offset, mean, variance, epsilon, data_format, is_training, name)\u001b[0m\n\u001b[0;32m   4084\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4085\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4086\u001b[1;33m       \u001b[0m_six\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4087\u001b[0m   \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4088\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python35\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: Could not find valid device for node.\nNode: {{node FusedBatchNorm}}\nAll kernels registered for op FusedBatchNorm :\n  device='CPU'; T in [DT_FLOAT]\n [Op:FusedBatchNorm]"
     ]
    }
   ],
   "source": [
    "num_train_data = train_x.shape[0]\n",
    "num_test_data = test_x.shape[0]\n",
    "\n",
    "status = checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "status.initialize_or_restore()\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    \n",
    "    d_avg_loss = 0\n",
    "    g_avg_loss = 0 \n",
    "    train_step = 0\n",
    "    \n",
    "    \n",
    "    for images, labels in train_dataset:\n",
    "        step_batch_size = labels.shape[0]\n",
    "        z = np.random.uniform(size=[step_batch_size, 4])\n",
    "        c = np.argmax(labels, axis=1)\n",
    "        c = c.reshape([-1,1])\n",
    "        z = np.concatenate([z, c], axis=1)\n",
    " \n",
    "        # d update\n",
    "        d_grad = d_grads(gen, dis, z, images)\n",
    "        optimizer.apply_gradients(zip(d_grad, dis.variables))\n",
    "        d_loss = d_loss_fn(gen, dis, z, images)\n",
    "        \n",
    "        d_avg_loss += d_loss\n",
    "    \n",
    "        # g update\n",
    "        g_grad = g_grads(gen, dis ,z)\n",
    "        optimizer.apply_gradients(zip(g_grad, gen.variables))\n",
    "        g_loss = g_loss_fn(gen, dis, z)\n",
    "        \n",
    "        g_avg_loss += g_loss\n",
    "        \n",
    "    d_avg_loss /= train_step\n",
    "    g_avg_loss /= train_step\n",
    "    \n",
    "    print(\"#{}Epoch\".format(epoch+1))\n",
    "    print(\"d_avg_loss:{}\\tg_avg_loss:{}\".format(d_avg_loss, g_avg_loss))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
