{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "training_epochs = 6\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_dir = os.getcwd()\n",
    "ckpt_dir_name = 'checkpoints'\n",
    "model_dir_name = 'cnn_session_mode'\n",
    "\n",
    "checkpoint_dir = os.path.join(cur_dir, ckpt_dir_name, model_dir_name)\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "checkpoint_path = os.path.join(checkpoint_dir, model_dir_name+'.ckpt')\n",
    "## TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,) (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "mnist = keras.datasets.mnist\n",
    "\n",
    "(train_x, train_y), (test_x, test_y) = mnist.load_data()\n",
    "\n",
    "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1) (60000, 10) (10000, 28, 28, 1) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "train_x = train_x.astype(np.float32) / 255\n",
    "test_x = test_x.astype(np.float32) / 255\n",
    "\n",
    "train_x = np.expand_dims(train_x,3)\n",
    "test_x = np.expand_dims(test_x,3)\n",
    "\n",
    "train_y = to_categorical(train_y, 10)\n",
    "test_y = to_categorical(test_y, 10)\n",
    "\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y)).shuffle(buffer_size=70000).batch(batch_size)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_x, test_y)).batch(batch_size)\n",
    "\n",
    "num_train_data = train_x.shape[0]\n",
    "num_test_data = test_x.shape[0]\n",
    "\n",
    "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traditional Way\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "        self.Y = tf.placeholder(tf.float32, [None,10])\n",
    "        self.is_training = tf.placeholder(tf.bool, [])\n",
    "    \n",
    "    def create_model(self, reuse=False):\n",
    "        self.model = self._model(self.X, self.is_training, reuse=reuse)\n",
    "        \n",
    "    def _model(self, x, is_training=False, reuse=False):\n",
    "        with tf.variable_scope('model_layers', reuse=reuse):\n",
    "            conv1 = tf.contrib.layers.conv2d(x, 32, 3, padding='SAME', scope=\"conv1\", activation_fn=tf.nn.relu)\n",
    "            # (n, 28, 28, 1) => (n, 28, 28,32)\n",
    "            pool1 = tf.contrib.layers.max_pool2d(conv1, 2)\n",
    "            # (n, 28, 28, 32) => (n, 14, 14,32)\n",
    "            \n",
    "            conv2 = tf.contrib.layers.conv2d(pool1, 64, 3, padding='SAME',scope='conv2', activation_fn=None)\n",
    "            # (n, 14, 14, 32) => (n, 14, 14, 64)\n",
    "            batch2 = tf.contrib.layers.batch_norm(conv2, is_training=is_training, scope=\"batch2\", activation_fn=tf.nn.relu)\n",
    "            pool2 = tf.contrib.layers.max_pool2d(batch2, 2)\n",
    "            # (n, 14, 14, 64) => (n, 7, 7, 64)\n",
    "            \n",
    "            conv3 = tf.contrib.layers.conv2d(pool2, 128, 3, padding='SAME', scope='conv3', activation_fn=tf.nn.relu)\n",
    "            # (n, 7, 7, 64) => (n, 7, 7, 128)\n",
    "            flatten = tf.contrib.layers.flatten(conv3, scope='flatten')\n",
    "            flatten = tf.contrib.layers.dropout(flatten, keep_prob=0.7, is_training=is_training)\n",
    "            \n",
    "            # fully-connected-layer\n",
    "            fc_layer1 = tf.contrib.layers.fully_connected(flatten, 256, activation_fn=None, scope='fc_layer1')\n",
    "            batch4 = tf.contrib.layers.batch_norm(fc_layer1, is_training=is_training, scope='batch4', activation_fn=tf.nn.relu)\n",
    "            batch4 = tf.contrib.layers.dropout(batch4, keep_prob=0.7, is_training=is_training)\n",
    "            \n",
    "            output = tf.contrib.layers.fully_connected(batch4, 10, activation_fn=None, scope='output_layer')\n",
    "            \n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From c:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From c:\\program files\\python35\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From c:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "cnn = Model()\n",
    "cnn.create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=cnn.model, labels= cnn.Y))\n",
    "\n",
    "pred = tf.equal(tf.argmax(cnn.model,1), tf.argmax(cnn.Y, 1))\n",
    "acc = tf.reduce_mean(tf.cast(pred, tf.float32))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, epsilon=0.01)\n",
    "\n",
    "var_list=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='model_layers')\n",
    "\n",
    "grads_and_vars = optimizer.compute_gradients(loss, var_list = var_list)\n",
    "train = optimizer.apply_gradients(grads_and_vars)\n",
    "\n",
    "\n",
    "saver = tf.train.Saver(var_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "num_train_data = train_x.shape[0]\n",
    "num_test_data = test_x.shape[0]\n",
    "\n",
    "train_iterator = train_dataset.make_initializable_iterator()\n",
    "test_iterator = test_dataset.make_initializable_iterator()\n",
    "\n",
    "tr_x, tr_y = train_iterator.get_next()\n",
    "ts_x, ts_y = test_iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "Failed to resotre variables from checkpoints\n",
      "1th Epoch\n",
      "train_avg_loss: 0.17217717277933844\t train_avg_acc: 0.9493166678460936\n",
      "2th Epoch\n",
      "train_avg_loss: 0.056719522549925995\t train_avg_acc: 0.9833500038584073\n",
      "3th Epoch\n",
      "train_avg_loss: 0.04086129269960414\t train_avg_acc: 0.9872000045577685\n",
      "4th Epoch\n",
      "train_avg_loss: 0.0331407292530154\t train_avg_acc: 0.9898666708171368\n",
      "5th Epoch\n",
      "train_avg_loss: 0.026944331123522715\t train_avg_acc: 0.9914500041306019\n",
      "test_avg_acc: 0.8620999950170517\n",
      "6th Epoch\n",
      "train_avg_loss: 0.02363658334109156\t train_avg_acc: 0.9923500040173531\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    try:\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "        print('Variables are succesfully resotored')\n",
    "    except:\n",
    "        sess.run(init)\n",
    "        print('Failed to resotre variables from checkpoints')\n",
    "        \n",
    "    for epoch in range(training_epochs):\n",
    "        avg_loss = 0\n",
    "        avg_train_acc = 0\n",
    "        avg_test_acc = 0\n",
    "        train_step = 0\n",
    "        test_step = 0\n",
    "        \n",
    "        \n",
    "        # training set\n",
    "        sess.run(train_iterator.initializer)\n",
    "        for _ in range(num_train_data//batch_size):\n",
    "            train_inputs, train_labels = sess.run([tr_x, tr_y])\n",
    "            feed_dict={cnn.X:train_inputs, cnn.Y: train_labels, cnn.is_training:True}\n",
    "            step_loss, step_acc, _ = sess.run([loss, acc, train], feed_dict=feed_dict)\n",
    "            avg_loss += step_loss\n",
    "            avg_train_acc += step_acc\n",
    "            train_step += 1\n",
    "        avg_loss /= train_step\n",
    "        avg_train_acc /= train_step\n",
    "            \n",
    "        print(\"{}th Epoch\".format(epoch+1))\n",
    "        print(\"train_avg_loss: {}\\t train_avg_acc: {}\".format(avg_loss, avg_train_acc))\n",
    "        \n",
    "        if (epoch+1)%5 == 0:\n",
    "            # test_set\n",
    "            sess.run(test_iterator.initializer)\n",
    "            for _ in range(num_test_data//batch_size):\n",
    "                test_inputs, test_labels = sess.run([ts_x, ts_y])\n",
    "                step_acc = sess.run(acc, feed_dict={\n",
    "                    cnn.X:test_inputs, \n",
    "                    cnn.Y:test_labels,\n",
    "                    cnn.is_training:False})\n",
    "                avg_test_acc += step_acc\n",
    "                test_step += 1\n",
    "            avg_test_acc /= test_step\n",
    "            print('test_avg_acc: {}'.format(avg_test_acc))\n",
    "            saver.save(sess, checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
