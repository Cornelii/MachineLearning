{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH.9. GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/data/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist=input_data.read_data_sets('./mnist/data/',one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epoch=100\n",
    "batch_size=100\n",
    "learning_rate=0.0002\n",
    "n_hidden=256\n",
    "n_input=28*28\n",
    "n_noise=128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=tf.placeholder(tf.float32,[None,n_input])\n",
    "Z=tf.placeholder(tf.float32,[None,n_noise])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G_Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_W1=tf.Variable(tf.random_normal([n_noise,n_hidden],stddev=0.01))\n",
    "G_b1=tf.Variable(tf.zeros([n_hidden]))\n",
    "G_W2=tf.Variable(tf.random_normal([n_hidden,n_input],stddev=0.01))\n",
    "G_b2=tf.Variable(tf.zeros([n_input]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D_Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_W1=tf.Variable(tf.random_normal([n_input,n_hidden],stddev=0.01))\n",
    "D_b1=tf.Variable(tf.zeros([n_hidden]))\n",
    "D_W2=tf.Variable(tf.random_normal([n_hidden,1],stddev=0.01))\n",
    "D_b2=tf.Variable(tf.zeros([1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(noise_z):\n",
    "    hidden=tf.nn.relu(tf.matmul(noise_z,G_W1)+G_b1)\n",
    "    output=tf.nn.sigmoid(tf.matmul(hidden,G_W2)+G_b2)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def discriminator(inputs):\n",
    "    hidden=tf.nn.relu(tf.matmul(inputs,D_W1)+D_b1)\n",
    "    output=tf.nn.sigmoid(tf.matmul(hidden,D_W2)+D_b2)\n",
    "    \n",
    "    return output\n",
    "\n",
    "#noise generator\n",
    "def get_noise(batch_size,n_noies):\n",
    "    return np.random.normal(size=(batch_size,n_noise))\n",
    "\n",
    "G=generator(Z)\n",
    "D_gene=discriminator(G)\n",
    "D_real=discriminator(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_D=tf.reduce_mean(tf.log(D_real)+tf.log(1-D_gene))\n",
    "loss_G=tf.reduce_mean(tf.log(D_gene))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_var_list=[D_W1,D_b1,D_W2,D_b2]\n",
    "G_var_list=[G_W1,G_b1,G_W2,G_b2]\n",
    "\n",
    "train_D=tf.train.AdamOptimizer(learning_rate).minimize(-loss_D,var_list=D_var_list)  ## minus sign is for maximization\n",
    "train_G=tf.train.AdamOptimizer(learning_rate).minimize(-loss_G,var_list=G_var_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "total_batch=int(mnist.train.num_examples/batch_size)\n",
    "loss_val_D,loss_val_G=0,0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 D loss: -0.3069 G loss: -2.381\n",
      "Epoch: 0002 D loss: -0.1007 G loss: -3.224\n",
      "Epoch: 0003 D loss: -0.6004 G loss: -1.551\n",
      "Epoch: 0004 D loss: -0.3725 G loss: -1.694\n",
      "Epoch: 0005 D loss: -0.5326 G loss: -1.831\n",
      "Epoch: 0006 D loss: -0.36 G loss: -2.184\n",
      "Epoch: 0007 D loss: -0.3415 G loss: -2.438\n",
      "Epoch: 0008 D loss: -0.3073 G loss: -2.744\n",
      "Epoch: 0009 D loss: -0.311 G loss: -2.218\n",
      "Epoch: 0010 D loss: -0.4089 G loss: -2.302\n",
      "Epoch: 0011 D loss: -0.4294 G loss: -2.354\n",
      "Epoch: 0012 D loss: -0.5052 G loss: -2.048\n",
      "Epoch: 0013 D loss: -0.4266 G loss: -1.905\n",
      "Epoch: 0014 D loss: -0.4111 G loss: -2.13\n",
      "Epoch: 0015 D loss: -0.5616 G loss: -1.708\n",
      "Epoch: 0016 D loss: -0.5562 G loss: -2.141\n",
      "Epoch: 0017 D loss: -0.4431 G loss: -2.38\n",
      "Epoch: 0018 D loss: -0.4699 G loss: -2.405\n",
      "Epoch: 0019 D loss: -0.4736 G loss: -2.407\n",
      "Epoch: 0020 D loss: -0.3418 G loss: -2.519\n",
      "Epoch: 0021 D loss: -0.4724 G loss: -2.166\n",
      "Epoch: 0022 D loss: -0.5251 G loss: -2.153\n",
      "Epoch: 0023 D loss: -0.6858 G loss: -2.069\n",
      "Epoch: 0024 D loss: -0.4428 G loss: -2.299\n",
      "Epoch: 0025 D loss: -0.6969 G loss: -2.455\n",
      "Epoch: 0026 D loss: -0.5125 G loss: -2.14\n",
      "Epoch: 0027 D loss: -0.5591 G loss: -2.204\n",
      "Epoch: 0028 D loss: -0.4925 G loss: -2.272\n",
      "Epoch: 0029 D loss: -0.5193 G loss: -2.386\n",
      "Epoch: 0030 D loss: -0.5173 G loss: -2.183\n",
      "Epoch: 0031 D loss: -0.6408 G loss: -2.099\n",
      "Epoch: 0032 D loss: -0.6596 G loss: -2.428\n",
      "Epoch: 0033 D loss: -0.5585 G loss: -2.113\n",
      "Epoch: 0034 D loss: -0.6668 G loss: -2.243\n",
      "Epoch: 0035 D loss: -0.6126 G loss: -2.309\n",
      "Epoch: 0036 D loss: -0.5198 G loss: -2.18\n",
      "Epoch: 0037 D loss: -0.4869 G loss: -2.272\n",
      "Epoch: 0038 D loss: -0.583 G loss: -2.226\n",
      "Epoch: 0039 D loss: -0.7476 G loss: -2.37\n",
      "Epoch: 0040 D loss: -0.6645 G loss: -2.249\n",
      "Epoch: 0041 D loss: -0.5724 G loss: -2.256\n",
      "Epoch: 0042 D loss: -0.7006 G loss: -2.395\n",
      "Epoch: 0043 D loss: -0.7128 G loss: -2.103\n",
      "Epoch: 0044 D loss: -0.5989 G loss: -2.092\n",
      "Epoch: 0045 D loss: -0.6235 G loss: -1.899\n",
      "Epoch: 0046 D loss: -0.6698 G loss: -2.181\n",
      "Epoch: 0047 D loss: -0.6286 G loss: -1.963\n",
      "Epoch: 0048 D loss: -0.8662 G loss: -1.948\n",
      "Epoch: 0049 D loss: -0.5323 G loss: -2.049\n",
      "Epoch: 0050 D loss: -0.7263 G loss: -2.034\n",
      "Epoch: 0051 D loss: -0.6567 G loss: -2.054\n",
      "Epoch: 0052 D loss: -0.7117 G loss: -1.896\n",
      "Epoch: 0053 D loss: -0.4827 G loss: -2.111\n",
      "Epoch: 0054 D loss: -0.6381 G loss: -1.996\n",
      "Epoch: 0055 D loss: -0.7266 G loss: -1.815\n",
      "Epoch: 0056 D loss: -0.6577 G loss: -1.927\n",
      "Epoch: 0057 D loss: -0.7467 G loss: -2.042\n",
      "Epoch: 0058 D loss: -0.8047 G loss: -1.915\n",
      "Epoch: 0059 D loss: -0.6394 G loss: -1.956\n",
      "Epoch: 0060 D loss: -0.7916 G loss: -1.698\n",
      "Epoch: 0061 D loss: -0.6368 G loss: -2.074\n",
      "Epoch: 0062 D loss: -0.7582 G loss: -1.906\n",
      "Epoch: 0063 D loss: -0.7895 G loss: -1.948\n",
      "Epoch: 0064 D loss: -0.6449 G loss: -1.778\n",
      "Epoch: 0065 D loss: -0.838 G loss: -1.979\n",
      "Epoch: 0066 D loss: -0.7363 G loss: -1.75\n",
      "Epoch: 0067 D loss: -0.7094 G loss: -1.637\n",
      "Epoch: 0068 D loss: -0.8548 G loss: -1.707\n",
      "Epoch: 0069 D loss: -0.8376 G loss: -1.856\n",
      "Epoch: 0070 D loss: -0.7341 G loss: -1.795\n",
      "Epoch: 0071 D loss: -0.8927 G loss: -1.774\n",
      "Epoch: 0072 D loss: -0.7831 G loss: -1.679\n",
      "Epoch: 0073 D loss: -0.7345 G loss: -1.746\n",
      "Epoch: 0074 D loss: -0.8425 G loss: -1.885\n",
      "Epoch: 0075 D loss: -0.7962 G loss: -1.882\n",
      "Epoch: 0076 D loss: -0.7475 G loss: -1.831\n",
      "Epoch: 0077 D loss: -0.6823 G loss: -2.058\n",
      "Epoch: 0078 D loss: -0.7787 G loss: -1.953\n",
      "Epoch: 0079 D loss: -0.8764 G loss: -1.762\n",
      "Epoch: 0080 D loss: -0.8263 G loss: -1.86\n",
      "Epoch: 0081 D loss: -0.7206 G loss: -1.768\n",
      "Epoch: 0082 D loss: -0.7999 G loss: -1.873\n",
      "Epoch: 0083 D loss: -0.7244 G loss: -1.848\n",
      "Epoch: 0084 D loss: -0.7117 G loss: -1.77\n",
      "Epoch: 0085 D loss: -0.6442 G loss: -1.975\n",
      "Epoch: 0086 D loss: -0.8228 G loss: -1.789\n",
      "Epoch: 0087 D loss: -0.6875 G loss: -1.71\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(total_epoch):\n",
    "    for jmi in range(total_batch):\n",
    "        batch_xs,batch_ys=mnist.train.next_batch(batch_size)\n",
    "        noise=get_noise(batch_size,n_noise)\n",
    "        \n",
    "        _,loss_val_D=sess.run([train_D,loss_D],feed_dict={X:batch_xs,Z:noise})\n",
    "        _,loss_val_G=sess.run([train_G,loss_G],feed_dict={Z:noise})\n",
    "        \n",
    "    print('Epoch:','%04d'%(epoch+1),'D loss: {:.4}'.format(loss_val_D),'G loss: {:.4}'.format(loss_val_G))\n",
    "    \n",
    "    if epoch ==0 or (epoch+1)%10==0:\n",
    "        sample_size=10\n",
    "        noise=get_noise(sample_size,n_noise)\n",
    "        samples=sess.run(G,feed_dict={Z:noise})\n",
    "        \n",
    "        fig,ax=plt.subplots(1,sample_size,figsize=(sample_size,1))\n",
    "        \n",
    "        for jmi in range(sample_size):\n",
    "            ax[jmi].set_axis_off()\n",
    "            ax[jmi].imshow(np.reshape(samples[jmi],(28,28)))\n",
    "            \n",
    "        plt.savefig('samples/{}.png'.format(str(epoch).zfill(3)),bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "print('Optimization has been done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
