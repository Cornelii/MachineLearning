{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN2 (generate numbers what i induce) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/data/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist=input_data.read_data_sets('./mnist/data/',one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epoch=100\n",
    "batch_size=100\n",
    "n_hidden=256\n",
    "n_input=28*28\n",
    "n_noise=128\n",
    "n_class=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=tf.placeholder(tf.float32,[None,n_input])\n",
    "Y=tf.placeholder(tf.float32,[None,n_class])\n",
    "Z=tf.placeholder(tf.float32,[None,n_noise])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(noise,labels):\n",
    "    with tf.variable_scope('generator'):\n",
    "        inputs=tf.concat([noise,labels],1)\n",
    "        hidden=tf.layers.dense(inputs,n_hidden,activation=tf.nn.relu)\n",
    "        output=tf.layers.dense(hidden,n_input,activation=tf.nn.sigmoid)\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(inputs,labels,reuse=None):\n",
    "    with tf.variable_scope('discriminator') as scope:\n",
    "        if reuse:\n",
    "            scope.reuse_variables()\n",
    "            \n",
    "        inputs=tf.concat([inputs,labels],1)\n",
    "        \n",
    "        hidden=tf.layers.dense(inputs,n_hidden,activation=tf.nn.relu)\n",
    "        output=tf.layers.dense(hidden,1,activation=None)\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noise(batch_size,n_noise):\n",
    "    return np.random.uniform(-1.,1.,size=[batch_size,n_noise])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=generator(Z,Y)\n",
    "D_real=discriminator(X,Y)\n",
    "D_gene=discriminator(G,Y,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_D_real=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_real,labels=tf.ones_like(D_real)))\n",
    "loss_D_gene=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_gene,labels=tf.zeros_like(D_gene)))\n",
    "loss_D=loss_D_real+loss_D_gene ## minimization this\n",
    "\n",
    "loss_G=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_gene,labels=tf.ones_like(D_gene)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_D=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope='discriminator')\n",
    "vars_G=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope='generator')\n",
    "train_D=tf.train.AdamOptimizer().minimize(loss_D,var_list=vars_D)\n",
    "train_G=tf.train.AdamOptimizer().minimize(loss_G,var_list=vars_G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 D loss: 0.0051 G loss: 8.9146\n",
      "Epoch: 0002 D loss: 0.0110 G loss: 8.5142\n",
      "Epoch: 0003 D loss: 0.0032 G loss: 10.0568\n",
      "Epoch: 0004 D loss: 0.0136 G loss: 7.4542\n",
      "Epoch: 0005 D loss: 0.0271 G loss: 6.7865\n",
      "Epoch: 0006 D loss: 0.0395 G loss: 6.6287\n",
      "Epoch: 0007 D loss: 0.0592 G loss: 6.4998\n",
      "Epoch: 0008 D loss: 0.0199 G loss: 6.9851\n",
      "Epoch: 0009 D loss: 0.0883 G loss: 6.3330\n",
      "Epoch: 0010 D loss: 0.0892 G loss: 6.3971\n",
      "Epoch: 0011 D loss: 0.2505 G loss: 5.6947\n",
      "Epoch: 0012 D loss: 0.1847 G loss: 4.7563\n",
      "Epoch: 0013 D loss: 0.5095 G loss: 3.9340\n",
      "Epoch: 0014 D loss: 0.2278 G loss: 4.2248\n",
      "Epoch: 0015 D loss: 0.2679 G loss: 4.8708\n",
      "Epoch: 0016 D loss: 0.3890 G loss: 3.8491\n",
      "Epoch: 0017 D loss: 0.3961 G loss: 3.9635\n",
      "Epoch: 0018 D loss: 0.5029 G loss: 4.1040\n",
      "Epoch: 0019 D loss: 0.3511 G loss: 4.0852\n",
      "Epoch: 0020 D loss: 0.7132 G loss: 3.6390\n",
      "Epoch: 0021 D loss: 0.5525 G loss: 3.7963\n",
      "Epoch: 0022 D loss: 0.6633 G loss: 2.9266\n",
      "Epoch: 0023 D loss: 0.7792 G loss: 2.3561\n",
      "Epoch: 0024 D loss: 0.7775 G loss: 3.0207\n",
      "Epoch: 0025 D loss: 0.7735 G loss: 2.8477\n",
      "Epoch: 0026 D loss: 0.7099 G loss: 2.3371\n",
      "Epoch: 0027 D loss: 0.5162 G loss: 2.7891\n",
      "Epoch: 0028 D loss: 0.6653 G loss: 2.7283\n",
      "Epoch: 0029 D loss: 0.5729 G loss: 2.3727\n",
      "Epoch: 0030 D loss: 0.7922 G loss: 2.6236\n",
      "Epoch: 0031 D loss: 0.7217 G loss: 2.6485\n",
      "Epoch: 0032 D loss: 0.6956 G loss: 2.5091\n",
      "Epoch: 0033 D loss: 0.6471 G loss: 2.2447\n",
      "Epoch: 0034 D loss: 0.9580 G loss: 2.0482\n",
      "Epoch: 0035 D loss: 0.5857 G loss: 2.6490\n",
      "Epoch: 0036 D loss: 0.9074 G loss: 2.1015\n",
      "Epoch: 0037 D loss: 0.8634 G loss: 1.8601\n",
      "Epoch: 0038 D loss: 0.7436 G loss: 2.1110\n",
      "Epoch: 0039 D loss: 0.7517 G loss: 2.6149\n",
      "Epoch: 0040 D loss: 0.7083 G loss: 2.4081\n",
      "Epoch: 0041 D loss: 0.7238 G loss: 2.3686\n",
      "Epoch: 0042 D loss: 0.6866 G loss: 2.0979\n",
      "Epoch: 0043 D loss: 0.9307 G loss: 1.6493\n",
      "Epoch: 0044 D loss: 0.7307 G loss: 2.3982\n",
      "Epoch: 0045 D loss: 0.7506 G loss: 2.2478\n",
      "Epoch: 0046 D loss: 0.5238 G loss: 2.3641\n",
      "Epoch: 0047 D loss: 0.6752 G loss: 2.3346\n",
      "Epoch: 0048 D loss: 0.5916 G loss: 2.4178\n",
      "Epoch: 0049 D loss: 0.6962 G loss: 2.4285\n",
      "Epoch: 0050 D loss: 0.6196 G loss: 2.2944\n",
      "Epoch: 0051 D loss: 0.6893 G loss: 2.1454\n",
      "Epoch: 0052 D loss: 0.7777 G loss: 1.9247\n",
      "Epoch: 0053 D loss: 0.6533 G loss: 2.3451\n",
      "Epoch: 0054 D loss: 0.7100 G loss: 2.5222\n",
      "Epoch: 0055 D loss: 0.7212 G loss: 1.9971\n",
      "Epoch: 0056 D loss: 0.6910 G loss: 2.3211\n",
      "Epoch: 0057 D loss: 0.6637 G loss: 2.2966\n",
      "Epoch: 0058 D loss: 0.7059 G loss: 2.4263\n",
      "Epoch: 0059 D loss: 0.6468 G loss: 2.2697\n",
      "Epoch: 0060 D loss: 0.6271 G loss: 2.5026\n",
      "Epoch: 0061 D loss: 0.7245 G loss: 2.0877\n",
      "Epoch: 0062 D loss: 0.9304 G loss: 2.1978\n",
      "Epoch: 0063 D loss: 0.7220 G loss: 2.2562\n",
      "Epoch: 0064 D loss: 0.9549 G loss: 1.8788\n",
      "Epoch: 0065 D loss: 0.6990 G loss: 2.0080\n",
      "Epoch: 0066 D loss: 0.6724 G loss: 2.1173\n",
      "Epoch: 0067 D loss: 0.7831 G loss: 2.2326\n",
      "Epoch: 0068 D loss: 0.7822 G loss: 1.6929\n",
      "Epoch: 0069 D loss: 0.6745 G loss: 2.1189\n",
      "Epoch: 0070 D loss: 0.7024 G loss: 2.6437\n",
      "Epoch: 0071 D loss: 0.6477 G loss: 2.0090\n",
      "Epoch: 0072 D loss: 0.8168 G loss: 2.0284\n",
      "Epoch: 0073 D loss: 0.6439 G loss: 2.3483\n",
      "Epoch: 0074 D loss: 0.8443 G loss: 1.8688\n",
      "Epoch: 0075 D loss: 0.5301 G loss: 2.2448\n",
      "Epoch: 0076 D loss: 0.7874 G loss: 2.2189\n",
      "Epoch: 0077 D loss: 0.7962 G loss: 2.2417\n",
      "Epoch: 0078 D loss: 0.5782 G loss: 2.5054\n",
      "Epoch: 0079 D loss: 0.8813 G loss: 1.9658\n",
      "Epoch: 0080 D loss: 0.6628 G loss: 1.8482\n",
      "Epoch: 0081 D loss: 0.5923 G loss: 2.3196\n",
      "Epoch: 0082 D loss: 0.7231 G loss: 2.1386\n",
      "Epoch: 0083 D loss: 0.8503 G loss: 1.9618\n",
      "Epoch: 0084 D loss: 0.8726 G loss: 1.9006\n",
      "Epoch: 0085 D loss: 0.8681 G loss: 2.0294\n",
      "Epoch: 0086 D loss: 0.7608 G loss: 1.8921\n",
      "Epoch: 0087 D loss: 0.5751 G loss: 2.0138\n",
      "Epoch: 0088 D loss: 0.8202 G loss: 1.7304\n",
      "Epoch: 0089 D loss: 0.8229 G loss: 1.7537\n",
      "Epoch: 0090 D loss: 0.7700 G loss: 2.2800\n",
      "Epoch: 0091 D loss: 0.9074 G loss: 1.7403\n",
      "Epoch: 0092 D loss: 0.6104 G loss: 2.4991\n",
      "Epoch: 0093 D loss: 0.9239 G loss: 1.8693\n",
      "Epoch: 0094 D loss: 0.8235 G loss: 2.0699\n",
      "Epoch: 0095 D loss: 0.7012 G loss: 2.0305\n",
      "Epoch: 0096 D loss: 0.8781 G loss: 1.7772\n"
     ]
    }
   ],
   "source": [
    "total_batch=int(mnist.train.num_examples/batch_size)\n",
    "loss_val_D,loss_val_G=0,0\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    for jmi in range(total_batch):\n",
    "        batch_xs,batch_ys=mnist.train.next_batch(batch_size)\n",
    "        noise=get_noise(batch_size,n_noise)\n",
    "        \n",
    "        _,loss_val_D=sess.run([train_D,loss_D],feed_dict={X:batch_xs,Y:batch_ys,Z:noise})\n",
    "        _,loss_val_G=sess.run([train_G,loss_G],feed_dict={Y:batch_ys,Z:noise})\n",
    "        \n",
    "    print('Epoch:','%04d'%(epoch+1),'D loss: {:.4f}'.format(loss_val_D),'G loss: {:.4f}'.format(loss_val_G))\n",
    "        \n",
    "    if epoch==0 or (epoch+1)%10==0:\n",
    "        sample_size=10\n",
    "        noise=get_noise(sample_size,n_noise)\n",
    "        samples=sess.run(G,feed_dict={Y:mnist.test.labels[:sample_size],Z:noise})\n",
    "\n",
    "        fig,ax=plt.subplots(2,10,figsize=(sample_size,2))\n",
    "        \n",
    "        for jmi in range(sample_size):\n",
    "            ax[0][jmi].set_axis_off()\n",
    "            ax[1][jmi].set_axis_off()\n",
    "            \n",
    "            ax[0][jmi].imshow(np.reshape(mnist.test.images[jmi],(28,28)))\n",
    "            ax[1][jmi].imshow(np.reshape(samples[jmi],(28,28)))\n",
    "            \n",
    "        plt.savefig('samples2/{}.png'.format(str(epoch).zfill(3)),bbox_inches='tight')\n",
    "        \n",
    "print('Optimization has been done!')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
